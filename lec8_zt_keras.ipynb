{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Boston Housing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet does the following:\n",
    "\n",
    "Imports the necessary libraries and modules.\n",
    "\n",
    "Loads the Boston Housing dataset and splits it into training and testing sets.\n",
    "\n",
    "Normalizes the data using StandardScaler.\n",
    "\n",
    "Defines a neural network model with two hidden layers, each with 64 units and ReLU activation.\n",
    "\n",
    "Compiles the model using the Adam optimizer and mean squared error (MSE) as the loss function.\n",
    "\n",
    "Trains the model on the training data for 100 epochs with a batch size of 16.\n",
    "\n",
    "Evaluates the model on the test data and prints the mean absolute error (MAE).\n",
    "\n",
    "Makes predictions on the test data and calculates the mean squared error (MSE) between the predictions and the true targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import boston_housing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Boston Housing dataset\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# Data normalization (Standardization)\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e10d845240>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_targets, epochs=100, batch_size=16, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mean absolute error: 2.5947375297546387\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Mean squared error: 17.361431111319817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = model.evaluate(test_data, test_targets, verbose=0)\n",
    "print(f\"Test mean absolute error: {test_mae}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(test_targets, predictions)\n",
    "print(f\"Mean squared error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
